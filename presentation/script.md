Method
S9: Now it is my turn to introduce method and results part to you.
S10: First of all, let's take a look at the method part. Namely, the algorithm we adopt in our project. We mainly use Q learning as the reinforcement learning algorithm to require the agent train a route based on the reward setting of the environment. The agent will continuously collect reward from its environment and gain more and more experience from the interaction witj the environment. Finally, it will get a better route, which can make the reward it gains from the environment greater.
How does it work in our environment? Let me introduce it to you. Above all, we will construct a Q table, with the size of 100*4, to indicate the quality of one action in a given node. Here we have 100 nodes in total, and 4 possible actions(up, down, right, left). Each Q-value answers the following question: How many reward will I expect to get if take this action at this node? After training the agent and get the model, then the agent can find out the best action he thinks with the trained Q value table.
Now let's take a look at the algorithm. Firstly, we initialize the Q table to contain all 0. Then, we will basically keep updating the Q table with exploration. Alpha is the learning rate, it measures how radical we are. Epsilon measures how willing we are to try random action even if we already have a best action from Q table. If you are interested, I can tell more details in Q&A session. Now let's move to the next part.
S11: Previously it is simply common RL. So how is our project different from that? We incorporate human feedback into the training process to improve the training quality. Now I'll tell you the principle of that. Please take a look at the left picture. As you can see, there're two kinds of route, red and green. Red routes are those trained by the agent from the RL algorithm, and the green route is manually set by we human. Like what is shown in the right user interface figure, we enforce it to go several steps. Finally, we allow it to train itself again, generating the last red route. Why would we bother doing this? I think you will know that very soon.
S12: Now let's look at the result part.
S13, 14: Here are the training results of the agent. We can easily find out that they are stuck between some high reward nodes and fail to reach the destination. Consequently, the average reward is low, which is around 1000. It means poor performance withour human feedback. Besides, we can observe that the agent can only go to the destination with the rate of 1.4%.
Why would that happen? Please carefully look at the environment figure. There are some high reward nodes connected with each other, and the nodes around destination have low reward, which will resist the agent from reaching the destination and be satisfied with moving between the two high reward nodes. like (0,3) and (0,4).
S15: How about the result after the involvement of human feedback. I would say it is not guaranteed to succeed, but at least it has a much higher successful rate. The final performance heavily relies on the quality of human feedback. What we are doing here is forcing the agent out of its comfort zone. It is risky. It may have three consequences. It may go to another comfort zone, go to the destination, or return to its original comfort zone. If we fails, like what happens in figure 1 and 3, then the final reward will be even smaller. This is quite understandable because the agent explores the outside environment and gains nothing. Remember, the positive reward get discounted with the increase of time, so it is natural for the reward to decrease.
Consequently, the design of human feedback in this case is extremely critical.
